---
title: "Introduction to Bayesian Modeling with PyMC"
subtitle: "PyCon Colombia 2022"
author: "Dr. Juan Orduz"
format:
  revealjs: 
    slide-number: false
    chalkboard: 
      buttons: false
    preview-links: auto
    theme:
        - "dark"
---

## Why Bayesian Modeling?

::: incremental
- Conceptrually transparent interpretation of probablity.
- Uncertainty quantification.
- Allows to explicitly include prior knowledge in the model.
- Felxible and suited for many applications in academia and industry.
- Scalable*
:::

## Why PyMC?


![](pyconco22_orduz_files/images/pymc4_announcement.png)

::: footer
Learn more: [PyMC 4.0 Release Announcement](https://www.pymc.io/blog/v4_announcement.html)
:::

## Bayesian Inference \: An Example

Suppose you see a person with long hair. You want to estimate the probablity that this person is a woman. That is, for $A = \text{woman}$ and $B = \text{long hair}$, we want to estimate $P(A|B)$

### `Prior-Information`

You belive $P(A) = 0.5$, $P(B)=0.4$ and $P(B|A) = 0.7$.

### `Bayes Rule`
$$
P(A|B) = \frac{P(A)\times P(B|A)}{P(B)} = \frac{0.5 \times 0.7}{0.4} = 0.875
$$

::: footer
[Introduction to Bayesian Modeling with PyMC3](https://juanitorduz.github.io/intro_pymc3/)
:::

## Some Examples\: Distributions

![](pymc_lr_files/pymc_lr_25_0.svg){fig-align="center"}

## Bayesian Approach to Data Analysis

Assume $y\sim p(y|\theta)$, where $\theta$ is a parameter(s) for the distribution (e.g. $y\sim N(\mu, \sigma^2)$). From Bayes Theorem:

$$
p(\theta|y)=\frac{p(y|\theta) \times p(\theta)}{p(y)} = 
\displaystyle{\frac{p(y|\theta)\times p(\theta)}{\color{red}{\int p(y|\theta)p(\theta)d\theta}}}
$$

::: incremental
- The function $p(y|\theta)$ is called the `likelihood`.
- $p(\theta)$ is the `prior` distribution of $\theta$.
:::

. . .

$$
p(\theta|y)  \propto \text{likelihood} \times \text{prior}.
$$

. . .

Integrals are **hard** to compute $\Longrightarrow$ we need `samplers`.


## Example \: Linear Regression

\begin{align*}
y & \sim \text{Normal}(\mu, \sigma^2)\\
\mu & = a + bx
\end{align*}

![](pymc_lr_files/pymc_lr_4_0.svg){fig-align="center"}

`Objective:` We want to estimate the (posterior) distributions of $a$, $b$ (and hence $\mu$) and $\sigma$ given $x$ and $y$.


## Model Specification: Math

::: columns
::: {.column width="50%"}
`Model Parametrization`:
\begin{align*}
y & \sim \text{Normal}(\mu, \sigma^2)\\
\mu & = a + bx \\
\end{align*}

`Prior Distributions`:
\begin{align*}
a & \sim \text{Normal}(0, 2)\\
b & \sim \text{Normal}(0, 2) \\
\sigma & \sim \text{HalfNormal}(2)
\end{align*}
:::
::: {.column width="50%"}
![](pymc_lr_files/pymc_lr_6_0.svg){fig-align="center"}
:::
:::


## Model Specification: PyMC

``` {.python code-line-numbers="1-18|1|3-5|7-10|12-13|15-18"}
with pm.Model(coords={"idx": range(n_train)}) as model:

    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```
. . .

`Compare to:`

[sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression)

## Prior Predictive Sampling

``` {.python code-line-numbers="1-2"}
with model:
    prior_predictive = pm.sample_prior_predictive(samples=100)
```

![](pymc_lr_files/pymc_lr_9_0.svg){fig-align="center"}

## Fit Model

``` {.python code-line-numbers="1-4"}
with model:
    idata = pm.sample(target_accept=0.8, draws=1_000, chains=4)
    posterior_predictive = pm.sample_posterior_predictive(trace=idata)
```

![](pymc_lr_files/pymc_lr_15_0.svg){fig-align="center"}

## Posterior Predictive (Training Set)

::: columns
::: {.column width="50%"}
![Posterior distribution of $\mu$.](pymc_lr_files/pymc_lr_17_0.svg)
:::
::: {.column width="50%"}
![Posterior distribution of $y$.](pymc_lr_files/pymc_lr_18_0.svg)
:::
:::


## Posterior Predictive (Test Set)

::: columns
::: {.column width="50%"}
![Posterior distribution of $\mu$.](pymc_lr_files/pymc_lr_22_0.svg)
:::
::: {.column width="50%"}
![Posterior distribution of $y$.](pymc_lr_files/pymc_lr_23_0.svg)
:::
:::

## Model Variations: Prior Constraints

``` {.python code-line-numbers="1-18|9"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.HalfNormal(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

. . .

`Compare to:`

[sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression) with *positive = True*.

## Model Variations: Regularization

``` {.python code-line-numbers="9"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Laplace(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

. . .

`Compare to:`

[sklearn.linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge)

## Model Variations: Robust Regression

``` {.python code-line-numbers="11|17-19"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    nu = pm.Gamma(name="nu", a=10, b=10)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.StudentT(
        name="likelihood", mu=mu, sigma=sigma, nu=nu, observed=y, dims="idx"
    )
```

. . . 

`Compare to:`

[sklearn.linear_model.HuberRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor)

## Model Variations: Correlation 

## Example Time Varying Coefficients

## Application: Media Mix Model

::: footer
See [Media Effect Estimation with PyMC: Adstock, Saturation & Diminishing Returns](https://juanitorduz.github.io/pymc_mmm/)
:::

## Application: Customer Lifetime Value

::: footer
See [BG/NBD Model in PyMC](https://juanitorduz.github.io/bg_nbd_pymc/) and [Gamma-Gamma Model of Monetary Value in PyMC](https://juanitorduz.github.io/gamma_gamma_pymc/)
:::

## Many More Examples

## References