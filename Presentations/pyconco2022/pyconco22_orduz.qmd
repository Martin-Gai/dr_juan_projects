---
title: "Introduction to Bayesian Modeling with PyMC"
subtitle: "PyCon Colombia 2022"
author: "Dr. Juan Orduz"
format:
  revealjs: 
    slide-number: false
    chalkboard: 
      buttons: false
    preview-links: auto
    theme:
        - "dark"
---

## Why Bayesian Modeling?

::: incremental
- Conceptually transparent interpretation of probablity.
- Uncertainty quantification.
- Allows to explicitly include prior knowledge in the model.
- Felxible and suited for many applications in academia and industry.
- Scalable*
:::

## Why PyMC?


![](pyconco22_orduz_files/images/pymc4_announcement.png)

::: footer
Learn more: [PyMC 4.0 Release Announcement](https://www.pymc.io/blog/v4_announcement.html)
:::

## Bayesian Inference \: An Example

Suppose you see a person with long hair. You want to estimate the probablity that this person is a woman. That is, for $A = \text{woman}$ and $B = \text{long hair}$, we want to estimate $P(A|B)$

### `Prior-Information`

You belive $P(A) = 0.5$, $P(B)=0.4$ and $P(B|A) = 0.7$.

### `Bayes Rule`
$$
P(A|B) = \frac{P(A)\times P(B|A)}{P(B)} = \frac{0.5 \times 0.7}{0.4} = 0.875
$$

::: footer
See [Introduction to Bayesian Modeling with PyMC3](https://juanitorduz.github.io/intro_pymc3/)
:::

## Some Examples\: Distributions

![](pymc_lr_files/pymc_lr_25_0.svg){fig-align="center"}

## Bayesian Approach to Data Analysis

Assume $y\sim p(y|\theta)$, where $\theta$ is a parameter(s) for the distribution (e.g. $y\sim N(\mu, \sigma^2)$). From Bayes Theorem:

$$
p(\theta|y)=\frac{p(y|\theta) \times p(\theta)}{p(y)} = 
\displaystyle{\frac{p(y|\theta)\times p(\theta)}{\color{red}{\int p(y|\theta)p(\theta)d\theta}}}
$$

::: incremental
- The function $p(y|\theta)$ is called the `likelihood`.
- $p(\theta)$ is the `prior` distribution of $\theta$.
:::

. . .

$$
p(\theta|y)  \propto \text{likelihood} \times \text{prior}.
$$

. . .

Integrals are **hard** to compute $\Longrightarrow$ we need `samplers`.


## Example \: Linear Regression

\begin{align*}
y & \sim \text{Normal}(\mu, \sigma^2)\\
\mu & = a + bx
\end{align*}

![](pymc_lr_files/pymc_lr_4_0.svg){fig-align="center"}

`Objective:` We want to estimate the (posterior) distributions of $a$, $b$ (and hence $\mu$) and $\sigma$ given $x$ and $y$.


## Model Specification: Math

::: columns
::: {.column width="50%"}
`Model Parametrization`:
\begin{align*}
y & \sim \text{Normal}(\mu, \sigma^2)\\
\mu & = a + bx \\
\end{align*}

`Prior Distributions`:
\begin{align*}
a & \sim \text{Normal}(0, 2)\\
b & \sim \text{Normal}(0, 2) \\
\sigma & \sim \text{HalfNormal}(2)
\end{align*}
:::
::: {.column width="50%"}
![](pymc_lr_files/pymc_lr_6_0.svg){fig-align="center"}
:::
:::

::: footer
See [simulation notebook](https://github.com/juanitorduz/website_projects/blob/master/Presentations/pyconco2022/pymc_lr.ipynb)
:::


## Model Specification: PyMC

``` {.python code-line-numbers="1-18|1|3-5|7-10|12-13|15-18"}
with pm.Model(coords={"idx": range(n_train)}) as model:

    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```
. . .

`Compare to:`

[sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression)

## Prior Predictive Sampling

``` {.python code-line-numbers="1-2"}
with model:
    prior_predictive = pm.sample_prior_predictive(samples=100)
```

![Prior samples before passing the data through the model.](pymc_lr_files/pymc_lr_9_0.svg){fig-align="center"}

## Fit Model

``` {.python code-line-numbers="1-4"}
with model:
    idata = pm.sample(target_accept=0.8, draws=1_000, chains=4)
    posterior_predictive = pm.sample_posterior_predictive(trace=idata)
```

![Posterior samples distriibution viia NUTS sampler in PyMC. For each parameter we run 4 iindependent chains with 1000 samples each.](pymc_lr_files/pymc_lr_16_1.svg){fig-align="center"}

## Posterior Predictive (Training Set)

::: columns
::: {.column width="50%"}
![Posterior distribution of $\mu$.](pymc_lr_files/pymc_lr_18_0.svg)
:::
::: {.column width="50%"}
![Posterior distribution of $y$.](pymc_lr_files/pymc_lr_19_0.svg)
:::
:::


## Posterior Predictive (Test Set)

::: columns
::: {.column width="50%"}
![Posterior distribution of $\mu$.](pymc_lr_files/pymc_lr_22_0.svg)
:::
::: {.column width="50%"}
![Posterior distribution of $y$.](pymc_lr_files/pymc_lr_23_0.svg)
:::
:::

## Model Variations: Prior Constraints

``` {.python code-line-numbers="1-18|9"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.HalfNormal(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

. . .

`Compare to:`

[sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression) with *positive = True*.

## Model Variations: Regularization

``` {.python code-line-numbers="9"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Laplace(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

. . .

`Compare to:`

[sklearn.linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge)

## Model Variations: Robust Regression

``` {.python code-line-numbers="11|17-19"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    nu = pm.Gamma(name="nu", a=10, b=10)
    
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    
    # --- Likelihood ---
    likelihood = pm.StudentT(
        name="likelihood", mu=mu, sigma=sigma, nu=nu, observed=y, dims="idx"
    )
```

. . . 

`Compare to:`

[sklearn.linear_model.HuberRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor)

## Example: Bike Rental Model

<center>
<img src="bikes_pymc_files/cnt_temp.png"  width="1000">
</center>

::: footer
See [Exploring Tools for Interpretable Machine Learning](https://juanitorduz.github.io/interpretable_ml/)
:::

## Example: Bike Rental Model

### Linear Regression Baseline

<center>
<img src="bikes_pymc_files/bikes_pymc_12_0.svg" width="1000">
</center>

$$
\text{cnt} = \text{intercept} + b_{\text{temp}}\text{temp} + \cdots 
$$

::: footer
See [Exploring Tools for Interpretable Machine Learning](https://juanitorduz.github.io/interpretable_ml/)
:::


## Example: Bike Rental Model

### Linear Regression Baseline

<center>
<img src="bikes_pymc_files/bikes_pymc_25_0.svg"  width="1000">
</center>

::: footer
See [Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC](https://juanitorduz.github.io/bikes_pymc/)
:::


## Example: Bike Rental Model

### Two ML Models: Linear Regression ($L^1$) with second order interactions and XGBoost

<center>
<img src="bikes_pymc_files/interpretable_ml_104_0.svg"  width="850">
</center>

::: footer
See [Exploring Tools for Interpretable Machine Learning](https://juanitorduz.github.io/interpretable_ml/)
:::

## Example: Bike Rental Model

Two ML Models: Both see a negative effect of temperature in bke rentals ini the month of July.

<center>
<img src="bikes_pymc_files/interpretable_ml_106_0.svg"  width="600">
</center>


::: footer
See [Exploring Tools for Interpretable Machine Learning](https://juanitorduz.github.io/interpretable_ml/)
:::

## Example: Bike Rental Model

### Time-Varying Coefficients

<center>
<img src="bikes_pymc_files/bikes_pymc_29_0.svg"  width="1000">
</center>

$$
b(t) \sim N(b(t - 1), \sigma^2)
$$

::: footer
See [Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC](https://juanitorduz.github.io/bikes_pymc/)
:::

## Example: Bike Rental Model

### Time-Varying Coefficients

<center>
<img src="bikes_pymc_files/bikes_pymc_44_0.svg"  width="1000">
</center>

::: footer
See [Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC](https://juanitorduz.github.io/bikes_pymc/)
:::

## Example: Bike Rental Model

### Time-Varying Coefficients

<center>
<img src="bikes_pymc_files/bikes_pymc_48_0.svg"  width="900">
</center>

::: footer
See [Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC](https://juanitorduz.github.io/bikes_pymc/)
:::

## Example: Bike Rental Model

### Time-Varying Coefficients

![Effect of temperature on bike rentals as a function of tmie for a time varying coeffiicient model (via Gaussian random walk).](bikes_pymc_files/bikes_pymc_50_0.svg)

::: footer
See [Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC](https://juanitorduz.github.io/bikes_pymc/)
:::

## Application: Media Mix Model (MMM)

![](pyconco22_orduz_files/images/z_effect.png){fig-align="center"}

::: footer
See [Media Effect Estimation with PyMC: Adstock, Saturation & Diminishing Returns](https://juanitorduz.github.io/pymc_mmm/)
:::

## Application: Media Mix Model (MMM)

![MMM structure: Media data (cost, impressions or clicks) is modeled using **carryover effects** (adstock) and **saturation** effects. In addition, one can control for seasonality and external regressors. In this example, we allow **time-varying coefficients** to capture the effect development over time.](pyconco22_orduz_files/images/pymc_mmm_67_0.svg){fig-align="center"}

::: footer
See [Media Effect Estimation with PyMC: Adstock, Saturation & Diminishing Returns](https://juanitorduz.github.io/pymc_mmm/)
:::

## Application: Media Mix Model (MMM)

::: columns
::: {.column width="45%"}
![Fitted saturation effect. This can be used for mediia mix burged optimization.](pyconco22_orduz_files/images/lambda_fit.png){fig-align="center"}
:::
::: {.column width="55%"}
![Estimated effect varying over time.](pyconco22_orduz_files/images/z_effect_fitted.png){fig-align="center"}
:::
:::

::: footer
See [Media Effect Estimation with PyMC: Adstock, Saturation & Diminishing Returns](https://juanitorduz.github.io/pymc_mmm/)
:::

## Many More Examples and Applications!

![](pyconco22_orduz_files/images/pymc_examples.png){fig-align="center"}

::: footer
See [PyMC Documentation: Example Gallery](https://www.pymc.io/projects/examples/en/latest/gallery.html)
:::

## References

### Theory

- [Statistical Rethinking by Richard McElreath](https://xcelab.net/rm/statistical-rethinking/)
    - [Statistical Rethinking in PyMC](https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking_2)

### PyMC

- [PyMC Documentation: Example Gallery](https://www.pymc.io/projects/examples/en/latest/gallery.html)

### Use cases

- [https://learnbayesstats.com/](https://learnbayesstats.com/)

## Thank you!

[juanitorduz.github.io](https://juanitorduz.github.io/)

<center>
<img src="pyconco22_orduz_files/images/juanitorduz.png" width="900">
</center>