---
title: "Introduction to Bayesian Modeling with PyMC"
subtitle: "PyCon Colombia 2022"
author: "Dr. Juan Orduz"
format:
  revealjs: 
    slide-number: false
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: <https://juanitorduz.github.io>
    theme:
        - "dark"
---

## Why Bayesian Modeling?

::: incremental
- Conceptrually transparent interpretation of probablity.
- Uncertainty quantification.
- Allows to explicitly include prior knowledge in the model.
- Felxible and suited for many applications in academia and industry.
- Scalable*
:::

## Why PyMC?


![](pyconco22_orduz_files/images/pymc4_announcement.png)

::: footer
Learn more: [PyMC 4.0 Release Announcement](https://www.pymc.io/blog/v4_announcement.html)
:::

## Bayesian Inference \: An Example

Suppose you see a person with long hair. You want to estimate the probablity that this person is a woman. That is, for $A = \text{woman}$ and $B = \text{long hair}$, we want to estimate $P(A|B)$

### `Prior-Information`

You belive $P(A) = 0.5$, $P(B)=0.4$ and $P(B|A) = 0.7$.

### `Bayes Rule`
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)} = \frac{0.4 \times 0.7}{0.4} = 0.875
$$

::: footer
[Introduction to Bayesian Modeling with PyMC3](https://juanitorduz.github.io/intro_pymc3/)
:::

## Bayesian Approach to Data Analysis

Assume $y\sim p(y|\theta)$, where $\theta$ is a parameter for the distribution (e.g. $y\sim N(\mu, \sigma^2)$). From Bayes Theorem

$$
p(\theta|y)=\frac{p(y|\theta) \times p(\theta)}{p(y)} = 
\displaystyle{\frac{p(y|\theta)\times p(\theta)}{\color{red}{\int p(y|\theta)p(\theta)d\theta}}}
$$

- The function $p(y|\theta)$ is called the `likelihood`.
- $p(\theta)$ is the `prior` distribution of $\theta$.

$$
p(\theta|y)  \propto \text{likelihood} \times \text{prior}.
$$

Integrals are **hard** to compute $\Longrightarrow$ we need `samplers`.

::: footer
:::

## Example \: Linear Regression

\begin{align*}
y & \sim \text{Normal}(\mu, \sigma^2)\\
\mu & = a + bx
\end{align*}

![](pymc_lr_files/pymc_lr_3_0.svg)

::: footer
:::

## Model Spec 1

![](pymc_lr_files/pymc_lr_5_0.svg)

::: footer
:::

## Model Spec 2

``` {.python code-line-numbers="1-14|1|2-4|5-8|9-10|11-14"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

## Prior Predictive Sampling

``` {.python code-line-numbers="1-2"}
with model:
    prior_predictive = pm.sample_prior_predictive(samples=100)
```

![](pymc_lr_files/pymc_lr_8_0.svg)

## Fit Model

``` {.python code-line-numbers="1-4"}
with model:
    idata = pm.sample(target_accept=0.8, draws=1_000, chains=4)
    posterior_predictive = pm.sample_posterior_predictive(trace=idata)
```

![](pymc_lr_files/pymc_lr_14_0.svg)

## Posterior Predictive (Training Set)

![](pymc_lr_files/pymc_lr_16_0.svg)
![](pymc_lr_files/pymc_lr_17_0.svg)


## Posterior Predictive (Test Set)

![](pymc_lr_files/pymc_lr_21_0.svg)
![](pymc_lr_files/pymc_lr_22_0.svg)

## Model Variations: Prior Constraints

``` {.python code-line-numbers="1-14|7"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.HalfNormal(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

## Model Variations: Regularization

``` {.python code-line-numbers="1-14|7"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Laplace(name="b", sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    # --- Likelihood ---
    likelihood = pm.Normal(
        name="likelihood", mu=mu, sigma=sigma, observed=y, dims="idx"
    )
```

## Model Variations: Robust Regression

``` {.python code-line-numbers="1-15|9|12-15"}
with pm.Model(coords={"idx": range(n_train)}) as model:
    # --- Data Containers ---
    x = pm.MutableData(name="x", value=x_train)
    y = pm.MutableData(name="y", value=y_train)
    # --- Priors ---
    a = pm.Normal(name="a", mu=0, sigma=2)
    b = pm.Normal(name="b", mu=0, sigma=2)
    sigma = pm.HalfNormal(name="sigma", sigma=2)
    nu = pm.Gamma(name="nu", a=10, b=10)
    # --- Model Parametrization ---
    mu = pm.Deterministic(name="mu", var=a + b * x, dims="idx")
    # --- Likelihood ---
    likelihood = pm.StudentT(
        name="likelihood", mu=mu, sigma=sigma, nu=nu, observed=y, dims="idx"
    )
```